#trnasformer 
n_layers: 2                      # (int) The number of transformer layers in transformer encoder.
n_heads: 2                       # (int) The number of attention heads for multi-head attention layer.
hidden_size: 64                  # (int) The number of features in the hidden state
inner_size: 256                  # (int) The inner hidden size in feed-forward layer.
hidden_dropout_prob: 0.5         # (float) The probability of an element to be zeroed.
attn_dropout_prob: 0.5           # (float) The probability of an attention score to be zeroed.
hidden_act: 'gelu'               # (str) The activation function in feed-forward layer.
layer_norm_eps: 1e-12            # (float) A value added to the denominator for numerical stability.
initializer_range: 0.02          # (float) The standard deviation for normal initialization.

loss_type: 'CE'                  # (str) The type of loss function.

# moe
expert_num: 4                     # (int) The number of experts.


#对比学习
temperature: 1.0                 # (float) The temperature parameter for contrastive learning.
l1_weight: 1e-3                    # (float) The weight for L1 regularization.3
cl_weight: 1e-2                   # (float) The weight for contrastive learning loss.2
sim: "dot"
#mamba
d_state: 32                     # (int) SSM state expansion factor
d_conv: 4                       # (int) Local convolution width
expand: 2                       # (int) Block expansion factor